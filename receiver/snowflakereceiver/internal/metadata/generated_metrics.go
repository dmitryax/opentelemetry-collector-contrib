// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver"

	"github.com/open-telemetry/opentelemetry-collector-contrib/pkg/pdatautil"
)

type metricSnowflakeBillingCloudServiceTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.cloud_service.total metric with initial data.
func (m *metricSnowflakeBillingCloudServiceTotal) init() {
	m.data.SetName("snowflake.billing.cloud_service.total")
	m.data.SetDescription("Reported total credits used in the cloud service over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingCloudServiceTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingCloudServiceTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingCloudServiceTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingCloudServiceTotal(cfg MetricConfig) metricSnowflakeBillingCloudServiceTotal {
	m := metricSnowflakeBillingCloudServiceTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingTotalCreditTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.total_credit.total metric with initial data.
func (m *metricSnowflakeBillingTotalCreditTotal) init() {
	m.data.SetName("snowflake.billing.total_credit.total")
	m.data.SetDescription("Reported total credits used across account over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingTotalCreditTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingTotalCreditTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingTotalCreditTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingTotalCreditTotal(cfg MetricConfig) metricSnowflakeBillingTotalCreditTotal {
	m := metricSnowflakeBillingTotalCreditTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingVirtualWarehouseTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.virtual_warehouse.total metric with initial data.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) init() {
	m.data.SetName("snowflake.billing.virtual_warehouse.total")
	m.data.SetDescription("Reported total credits used by virtual warehouse service over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingVirtualWarehouseTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingVirtualWarehouseTotal(cfg MetricConfig) metricSnowflakeBillingVirtualWarehouseTotal {
	m := metricSnowflakeBillingVirtualWarehouseTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseCloudServiceTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.cloud_service.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.cloud_service.total")
	m.data.SetDescription("Credits used across cloud service for given warehouse over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseCloudServiceTotal(cfg MetricConfig) metricSnowflakeBillingWarehouseCloudServiceTotal {
	m := metricSnowflakeBillingWarehouseCloudServiceTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseTotalCreditTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.total_credit.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.total_credit.total")
	m.data.SetDescription("Total credits used associated with given warehouse over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseTotalCreditTotal(cfg MetricConfig) metricSnowflakeBillingWarehouseTotalCreditTotal {
	m := metricSnowflakeBillingWarehouseTotalCreditTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseVirtualWarehouseTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.virtual_warehouse.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.virtual_warehouse.total")
	m.data.SetDescription("Total credits used by virtual warehouse service for given warehouse over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseVirtualWarehouseTotal(cfg MetricConfig) metricSnowflakeBillingWarehouseVirtualWarehouseTotal {
	m := metricSnowflakeBillingWarehouseVirtualWarehouseTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseBytesScannedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.bytes_scanned.avg metric with initial data.
func (m *metricSnowflakeDatabaseBytesScannedAvg) init() {
	m.data.SetName("snowflake.database.bytes_scanned.avg")
	m.data.SetDescription("Average bytes scanned in a database over the last 24 hour window.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseBytesScannedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseBytesScannedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseBytesScannedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseBytesScannedAvg(cfg MetricConfig) metricSnowflakeDatabaseBytesScannedAvg {
	m := metricSnowflakeDatabaseBytesScannedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.query.count metric with initial data.
func (m *metricSnowflakeDatabaseQueryCount) init() {
	m.data.SetName("snowflake.database.query.count")
	m.data.SetDescription("Total query count for database over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseQueryCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseQueryCount(cfg MetricConfig) metricSnowflakeDatabaseQueryCount {
	m := metricSnowflakeDatabaseQueryCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeLoginsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.logins.total metric with initial data.
func (m *metricSnowflakeLoginsTotal) init() {
	m.data.SetName("snowflake.logins.total")
	m.data.SetDescription("Total login attempts for account over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeLoginsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("reported_client_type", reportedClientTypeAttributeValue)
	dp.Attributes().PutStr("is_success", isSuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeLoginsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeLoginsTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeLoginsTotal(cfg MetricConfig) metricSnowflakeLoginsTotal {
	m := metricSnowflakeLoginsTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakePipeCreditsUsedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.pipe.credits_used.total metric with initial data.
func (m *metricSnowflakePipeCreditsUsedTotal) init() {
	m.data.SetName("snowflake.pipe.credits_used.total")
	m.data.SetDescription("Snow pipe credits contotaled over the last 24 hour window.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakePipeCreditsUsedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("pipe_name", pipeNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakePipeCreditsUsedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakePipeCreditsUsedTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakePipeCreditsUsedTotal(cfg MetricConfig) metricSnowflakePipeCreditsUsedTotal {
	m := metricSnowflakePipeCreditsUsedTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBlocked struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.blocked metric with initial data.
func (m *metricSnowflakeQueryBlocked) init() {
	m.data.SetName("snowflake.query.blocked")
	m.data.SetDescription("Blocked query count for warehouse over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBlocked) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBlocked) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBlocked) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBlocked(cfg MetricConfig) metricSnowflakeQueryBlocked {
	m := metricSnowflakeQueryBlocked{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesDeletedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_deleted.avg metric with initial data.
func (m *metricSnowflakeQueryBytesDeletedAvg) init() {
	m.data.SetName("snowflake.query.bytes_deleted.avg")
	m.data.SetDescription("Average bytes deleted in database over the last 24 hour window.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesDeletedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesDeletedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesDeletedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesDeletedAvg(cfg MetricConfig) metricSnowflakeQueryBytesDeletedAvg {
	m := metricSnowflakeQueryBytesDeletedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledLocalAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.local.avg metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledLocalAvg) init() {
	m.data.SetName("snowflake.query.bytes_spilled.local.avg")
	m.data.SetDescription("Avergae bytes spilled (intermediate results do not fit in memory) by local storage over the last 24 hour window.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledLocalAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledLocalAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledLocalAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledLocalAvg(cfg MetricConfig) metricSnowflakeQueryBytesSpilledLocalAvg {
	m := metricSnowflakeQueryBytesSpilledLocalAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledRemoteAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.remote.avg metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledRemoteAvg) init() {
	m.data.SetName("snowflake.query.bytes_spilled.remote.avg")
	m.data.SetDescription("Avergae bytes spilled (intermediate results do not fit in memory) by remote storage over the last 24 hour window.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledRemoteAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledRemoteAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledRemoteAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledRemoteAvg(cfg MetricConfig) metricSnowflakeQueryBytesSpilledRemoteAvg {
	m := metricSnowflakeQueryBytesSpilledRemoteAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesWrittenAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_written.avg metric with initial data.
func (m *metricSnowflakeQueryBytesWrittenAvg) init() {
	m.data.SetName("snowflake.query.bytes_written.avg")
	m.data.SetDescription("Average bytes written by database over the last 24 hour window.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesWrittenAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesWrittenAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesWrittenAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesWrittenAvg(cfg MetricConfig) metricSnowflakeQueryBytesWrittenAvg {
	m := metricSnowflakeQueryBytesWrittenAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryCompilationTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.compilation_time.avg metric with initial data.
func (m *metricSnowflakeQueryCompilationTimeAvg) init() {
	m.data.SetName("snowflake.query.compilation_time.avg")
	m.data.SetDescription("Average time taken to compile query over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryCompilationTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryCompilationTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryCompilationTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryCompilationTimeAvg(cfg MetricConfig) metricSnowflakeQueryCompilationTimeAvg {
	m := metricSnowflakeQueryCompilationTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryDataScannedCacheAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.data_scanned_cache.avg metric with initial data.
func (m *metricSnowflakeQueryDataScannedCacheAvg) init() {
	m.data.SetName("snowflake.query.data_scanned_cache.avg")
	m.data.SetDescription("Average percentage of data scanned from cache over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryDataScannedCacheAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryDataScannedCacheAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryDataScannedCacheAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryDataScannedCacheAvg(cfg MetricConfig) metricSnowflakeQueryDataScannedCacheAvg {
	m := metricSnowflakeQueryDataScannedCacheAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecuted struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.executed metric with initial data.
func (m *metricSnowflakeQueryExecuted) init() {
	m.data.SetName("snowflake.query.executed")
	m.data.SetDescription("Executed query count for warehouse over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecuted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecuted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecuted) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecuted(cfg MetricConfig) metricSnowflakeQueryExecuted {
	m := metricSnowflakeQueryExecuted{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecutionTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.execution_time.avg metric with initial data.
func (m *metricSnowflakeQueryExecutionTimeAvg) init() {
	m.data.SetName("snowflake.query.execution_time.avg")
	m.data.SetDescription("Average time spent executing queries in database over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecutionTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecutionTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecutionTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecutionTimeAvg(cfg MetricConfig) metricSnowflakeQueryExecutionTimeAvg {
	m := metricSnowflakeQueryExecutionTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryPartitionsScannedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.partitions_scanned.avg metric with initial data.
func (m *metricSnowflakeQueryPartitionsScannedAvg) init() {
	m.data.SetName("snowflake.query.partitions_scanned.avg")
	m.data.SetDescription("Number of partitions scanned during query so far over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryPartitionsScannedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryPartitionsScannedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryPartitionsScannedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryPartitionsScannedAvg(cfg MetricConfig) metricSnowflakeQueryPartitionsScannedAvg {
	m := metricSnowflakeQueryPartitionsScannedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedOverload struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_overload metric with initial data.
func (m *metricSnowflakeQueryQueuedOverload) init() {
	m.data.SetName("snowflake.query.queued_overload")
	m.data.SetDescription("Overloaded query count for warehouse over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedOverload) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedOverload) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedOverload) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedOverload(cfg MetricConfig) metricSnowflakeQueryQueuedOverload {
	m := metricSnowflakeQueryQueuedOverload{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedProvision struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_provision metric with initial data.
func (m *metricSnowflakeQueryQueuedProvision) init() {
	m.data.SetName("snowflake.query.queued_provision")
	m.data.SetDescription("Number of compute resources queued for provisioning over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedProvision) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedProvision) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedProvision) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedProvision(cfg MetricConfig) metricSnowflakeQueryQueuedProvision {
	m := metricSnowflakeQueryQueuedProvision{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedOverloadTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_overload_time.avg metric with initial data.
func (m *metricSnowflakeQueuedOverloadTimeAvg) init() {
	m.data.SetName("snowflake.queued_overload_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue due to warehouse being overloaded over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedOverloadTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedOverloadTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedOverloadTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedOverloadTimeAvg(cfg MetricConfig) metricSnowflakeQueuedOverloadTimeAvg {
	m := metricSnowflakeQueuedOverloadTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedProvisioningTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_provisioning_time.avg metric with initial data.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) init() {
	m.data.SetName("snowflake.queued_provisioning_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue waiting for resources to provision over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedProvisioningTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedProvisioningTimeAvg(cfg MetricConfig) metricSnowflakeQueuedProvisioningTimeAvg {
	m := metricSnowflakeQueuedProvisioningTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedRepairTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_repair_time.avg metric with initial data.
func (m *metricSnowflakeQueuedRepairTimeAvg) init() {
	m.data.SetName("snowflake.queued_repair_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue waiting for compute resources to be repaired over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedRepairTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedRepairTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedRepairTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedRepairTimeAvg(cfg MetricConfig) metricSnowflakeQueuedRepairTimeAvg {
	m := metricSnowflakeQueuedRepairTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsDeletedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_deleted.avg metric with initial data.
func (m *metricSnowflakeRowsDeletedAvg) init() {
	m.data.SetName("snowflake.rows_deleted.avg")
	m.data.SetDescription("Number of rows deleted from a table (or tables) over the last 24 hour window.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsDeletedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsDeletedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsDeletedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsDeletedAvg(cfg MetricConfig) metricSnowflakeRowsDeletedAvg {
	m := metricSnowflakeRowsDeletedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsInsertedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_inserted.avg metric with initial data.
func (m *metricSnowflakeRowsInsertedAvg) init() {
	m.data.SetName("snowflake.rows_inserted.avg")
	m.data.SetDescription("Number of rows inserted into a table (or tables) over the last 24 hour window.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsInsertedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsInsertedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsInsertedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsInsertedAvg(cfg MetricConfig) metricSnowflakeRowsInsertedAvg {
	m := metricSnowflakeRowsInsertedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsProducedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_produced.avg metric with initial data.
func (m *metricSnowflakeRowsProducedAvg) init() {
	m.data.SetName("snowflake.rows_produced.avg")
	m.data.SetDescription("Average number of rows produced by statement over the last 24 hour window.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsProducedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsProducedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsProducedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsProducedAvg(cfg MetricConfig) metricSnowflakeRowsProducedAvg {
	m := metricSnowflakeRowsProducedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUnloadedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_unloaded.avg metric with initial data.
func (m *metricSnowflakeRowsUnloadedAvg) init() {
	m.data.SetName("snowflake.rows_unloaded.avg")
	m.data.SetDescription("Average number of rows unloaded during data export over the last 24 hour window.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUnloadedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUnloadedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUnloadedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUnloadedAvg(cfg MetricConfig) metricSnowflakeRowsUnloadedAvg {
	m := metricSnowflakeRowsUnloadedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUpdatedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_updated.avg metric with initial data.
func (m *metricSnowflakeRowsUpdatedAvg) init() {
	m.data.SetName("snowflake.rows_updated.avg")
	m.data.SetDescription("Average number of rows updated in a table over the last 24 hour window.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUpdatedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUpdatedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUpdatedAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUpdatedAvg(cfg MetricConfig) metricSnowflakeRowsUpdatedAvg {
	m := metricSnowflakeRowsUpdatedAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeSessionIDCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.session_id.count metric with initial data.
func (m *metricSnowflakeSessionIDCount) init() {
	m.data.SetName("snowflake.session_id.count")
	m.data.SetDescription("Distinct session id's associated with snowflake username over the last 24 hour window.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeSessionIDCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeSessionIDCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeSessionIDCount) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeSessionIDCount(cfg MetricConfig) metricSnowflakeSessionIDCount {
	m := metricSnowflakeSessionIDCount{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageFailsafeBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.failsafe_bytes.total metric with initial data.
func (m *metricSnowflakeStorageFailsafeBytesTotal) init() {
	m.data.SetName("snowflake.storage.failsafe_bytes.total")
	m.data.SetDescription("Number of bytes of data in Fail-safe.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageFailsafeBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageFailsafeBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageFailsafeBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageFailsafeBytesTotal(cfg MetricConfig) metricSnowflakeStorageFailsafeBytesTotal {
	m := metricSnowflakeStorageFailsafeBytesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.stage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStageBytesTotal) init() {
	m.data.SetName("snowflake.storage.stage_bytes.total")
	m.data.SetDescription("Number of bytes of stage storage used by files in all internal stages (named, table, user).")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStageBytesTotal(cfg MetricConfig) metricSnowflakeStorageStageBytesTotal {
	m := metricSnowflakeStorageStageBytesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStorageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.storage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStorageBytesTotal) init() {
	m.data.SetName("snowflake.storage.storage_bytes.total")
	m.data.SetDescription("Number of bytes of table storage used, including bytes for data currently in Time Travel.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStorageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStorageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStorageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStorageBytesTotal(cfg MetricConfig) metricSnowflakeStorageStorageBytesTotal {
	m := metricSnowflakeStorageStorageBytesTotal{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeTotalElapsedTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	config   MetricConfig   // metric config provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.total_elapsed_time.avg metric with initial data.
func (m *metricSnowflakeTotalElapsedTimeAvg) init() {
	m.data.SetName("snowflake.total_elapsed_time.avg")
	m.data.SetDescription("Average elapsed time over the last 24 hour window.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeTotalElapsedTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.config.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeTotalElapsedTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeTotalElapsedTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.config.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeTotalElapsedTimeAvg(cfg MetricConfig) metricSnowflakeTotalElapsedTimeAvg {
	m := metricSnowflakeTotalElapsedTimeAvg{config: cfg}
	if cfg.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// missedEmitsToDropRMB is number of missed emits after which resource builder will be dropped from MetricsBuilder.rmbMap.
// Potentially, this value can be made configurable through a MetricsBuilder option.
const missedEmitsToDropRMB = 5

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user config.
type MetricsBuilder struct {
	config    MetricsBuilderConfig                 // config of the metrics builder.
	buildInfo component.BuildInfo                  // contains version information
	startTime pcommon.Timestamp                    // start time that will be applied to all recorded data points.
	rmbMap    map[[16]byte]*ResourceMetricsBuilder // map of resource builders by resource hash.
}

type ResourceMetricsBuilder struct {
	buildInfo                                            component.BuildInfo
	startTime                                            pcommon.Timestamp // start time that will be applied to all recorded data points.
	metricsCapacity                                      int               // maximum observed number of metrics per resource.
	resource                                             pcommon.Resource
	missedEmits                                          int
	metricSnowflakeBillingCloudServiceTotal              metricSnowflakeBillingCloudServiceTotal
	metricSnowflakeBillingTotalCreditTotal               metricSnowflakeBillingTotalCreditTotal
	metricSnowflakeBillingVirtualWarehouseTotal          metricSnowflakeBillingVirtualWarehouseTotal
	metricSnowflakeBillingWarehouseCloudServiceTotal     metricSnowflakeBillingWarehouseCloudServiceTotal
	metricSnowflakeBillingWarehouseTotalCreditTotal      metricSnowflakeBillingWarehouseTotalCreditTotal
	metricSnowflakeBillingWarehouseVirtualWarehouseTotal metricSnowflakeBillingWarehouseVirtualWarehouseTotal
	metricSnowflakeDatabaseBytesScannedAvg               metricSnowflakeDatabaseBytesScannedAvg
	metricSnowflakeDatabaseQueryCount                    metricSnowflakeDatabaseQueryCount
	metricSnowflakeLoginsTotal                           metricSnowflakeLoginsTotal
	metricSnowflakePipeCreditsUsedTotal                  metricSnowflakePipeCreditsUsedTotal
	metricSnowflakeQueryBlocked                          metricSnowflakeQueryBlocked
	metricSnowflakeQueryBytesDeletedAvg                  metricSnowflakeQueryBytesDeletedAvg
	metricSnowflakeQueryBytesSpilledLocalAvg             metricSnowflakeQueryBytesSpilledLocalAvg
	metricSnowflakeQueryBytesSpilledRemoteAvg            metricSnowflakeQueryBytesSpilledRemoteAvg
	metricSnowflakeQueryBytesWrittenAvg                  metricSnowflakeQueryBytesWrittenAvg
	metricSnowflakeQueryCompilationTimeAvg               metricSnowflakeQueryCompilationTimeAvg
	metricSnowflakeQueryDataScannedCacheAvg              metricSnowflakeQueryDataScannedCacheAvg
	metricSnowflakeQueryExecuted                         metricSnowflakeQueryExecuted
	metricSnowflakeQueryExecutionTimeAvg                 metricSnowflakeQueryExecutionTimeAvg
	metricSnowflakeQueryPartitionsScannedAvg             metricSnowflakeQueryPartitionsScannedAvg
	metricSnowflakeQueryQueuedOverload                   metricSnowflakeQueryQueuedOverload
	metricSnowflakeQueryQueuedProvision                  metricSnowflakeQueryQueuedProvision
	metricSnowflakeQueuedOverloadTimeAvg                 metricSnowflakeQueuedOverloadTimeAvg
	metricSnowflakeQueuedProvisioningTimeAvg             metricSnowflakeQueuedProvisioningTimeAvg
	metricSnowflakeQueuedRepairTimeAvg                   metricSnowflakeQueuedRepairTimeAvg
	metricSnowflakeRowsDeletedAvg                        metricSnowflakeRowsDeletedAvg
	metricSnowflakeRowsInsertedAvg                       metricSnowflakeRowsInsertedAvg
	metricSnowflakeRowsProducedAvg                       metricSnowflakeRowsProducedAvg
	metricSnowflakeRowsUnloadedAvg                       metricSnowflakeRowsUnloadedAvg
	metricSnowflakeRowsUpdatedAvg                        metricSnowflakeRowsUpdatedAvg
	metricSnowflakeSessionIDCount                        metricSnowflakeSessionIDCount
	metricSnowflakeStorageFailsafeBytesTotal             metricSnowflakeStorageFailsafeBytesTotal
	metricSnowflakeStorageStageBytesTotal                metricSnowflakeStorageStageBytesTotal
	metricSnowflakeStorageStorageBytesTotal              metricSnowflakeStorageStorageBytesTotal
	metricSnowflakeTotalElapsedTimeAvg                   metricSnowflakeTotalElapsedTimeAvg
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(mbc MetricsBuilderConfig, settings receiver.CreateSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		config:    mbc,
		startTime: pcommon.NewTimestampFromTime(time.Now()),
		buildInfo: settings.BuildInfo,
		rmbMap:    make(map[[16]byte]*ResourceMetricsBuilder),
	}
	for _, opt := range options {
		opt(mb)
	}
	return mb
}

// resourceMetricsBuilderOption applies changes to provided resource metrics.
type resourceMetricsBuilderOption func(*ResourceMetricsBuilder)

// WithStartTimeOverride sets start time for all the resource metrics data points.
func WithStartTimeOverride(start pcommon.Timestamp) resourceMetricsBuilderOption {
	return func(rmb *ResourceMetricsBuilder) {
		rmb.startTime = start
	}
}

// ResourceMetricsBuilder returns a ResourceMetricsBuilder that can be used to record metrics for a specific resource.
// It requires Resource to be provided which should be built with ResourceBuilder.
func (mb *MetricsBuilder) ResourceMetricsBuilder(res pcommon.Resource, options ...resourceMetricsBuilderOption) *ResourceMetricsBuilder {
	hash := pdatautil.MapHash(res.Attributes())
	if rmb, ok := mb.rmbMap[hash]; ok {
		return rmb
	}
	rmb := &ResourceMetricsBuilder{
		startTime:                               mb.startTime,
		buildInfo:                               mb.buildInfo,
		resource:                                res,
		metricSnowflakeBillingCloudServiceTotal: newMetricSnowflakeBillingCloudServiceTotal(mb.config.Metrics.SnowflakeBillingCloudServiceTotal),
		metricSnowflakeBillingTotalCreditTotal:  newMetricSnowflakeBillingTotalCreditTotal(mb.config.Metrics.SnowflakeBillingTotalCreditTotal),
		metricSnowflakeBillingVirtualWarehouseTotal:          newMetricSnowflakeBillingVirtualWarehouseTotal(mb.config.Metrics.SnowflakeBillingVirtualWarehouseTotal),
		metricSnowflakeBillingWarehouseCloudServiceTotal:     newMetricSnowflakeBillingWarehouseCloudServiceTotal(mb.config.Metrics.SnowflakeBillingWarehouseCloudServiceTotal),
		metricSnowflakeBillingWarehouseTotalCreditTotal:      newMetricSnowflakeBillingWarehouseTotalCreditTotal(mb.config.Metrics.SnowflakeBillingWarehouseTotalCreditTotal),
		metricSnowflakeBillingWarehouseVirtualWarehouseTotal: newMetricSnowflakeBillingWarehouseVirtualWarehouseTotal(mb.config.Metrics.SnowflakeBillingWarehouseVirtualWarehouseTotal),
		metricSnowflakeDatabaseBytesScannedAvg:               newMetricSnowflakeDatabaseBytesScannedAvg(mb.config.Metrics.SnowflakeDatabaseBytesScannedAvg),
		metricSnowflakeDatabaseQueryCount:                    newMetricSnowflakeDatabaseQueryCount(mb.config.Metrics.SnowflakeDatabaseQueryCount),
		metricSnowflakeLoginsTotal:                           newMetricSnowflakeLoginsTotal(mb.config.Metrics.SnowflakeLoginsTotal),
		metricSnowflakePipeCreditsUsedTotal:                  newMetricSnowflakePipeCreditsUsedTotal(mb.config.Metrics.SnowflakePipeCreditsUsedTotal),
		metricSnowflakeQueryBlocked:                          newMetricSnowflakeQueryBlocked(mb.config.Metrics.SnowflakeQueryBlocked),
		metricSnowflakeQueryBytesDeletedAvg:                  newMetricSnowflakeQueryBytesDeletedAvg(mb.config.Metrics.SnowflakeQueryBytesDeletedAvg),
		metricSnowflakeQueryBytesSpilledLocalAvg:             newMetricSnowflakeQueryBytesSpilledLocalAvg(mb.config.Metrics.SnowflakeQueryBytesSpilledLocalAvg),
		metricSnowflakeQueryBytesSpilledRemoteAvg:            newMetricSnowflakeQueryBytesSpilledRemoteAvg(mb.config.Metrics.SnowflakeQueryBytesSpilledRemoteAvg),
		metricSnowflakeQueryBytesWrittenAvg:                  newMetricSnowflakeQueryBytesWrittenAvg(mb.config.Metrics.SnowflakeQueryBytesWrittenAvg),
		metricSnowflakeQueryCompilationTimeAvg:               newMetricSnowflakeQueryCompilationTimeAvg(mb.config.Metrics.SnowflakeQueryCompilationTimeAvg),
		metricSnowflakeQueryDataScannedCacheAvg:              newMetricSnowflakeQueryDataScannedCacheAvg(mb.config.Metrics.SnowflakeQueryDataScannedCacheAvg),
		metricSnowflakeQueryExecuted:                         newMetricSnowflakeQueryExecuted(mb.config.Metrics.SnowflakeQueryExecuted),
		metricSnowflakeQueryExecutionTimeAvg:                 newMetricSnowflakeQueryExecutionTimeAvg(mb.config.Metrics.SnowflakeQueryExecutionTimeAvg),
		metricSnowflakeQueryPartitionsScannedAvg:             newMetricSnowflakeQueryPartitionsScannedAvg(mb.config.Metrics.SnowflakeQueryPartitionsScannedAvg),
		metricSnowflakeQueryQueuedOverload:                   newMetricSnowflakeQueryQueuedOverload(mb.config.Metrics.SnowflakeQueryQueuedOverload),
		metricSnowflakeQueryQueuedProvision:                  newMetricSnowflakeQueryQueuedProvision(mb.config.Metrics.SnowflakeQueryQueuedProvision),
		metricSnowflakeQueuedOverloadTimeAvg:                 newMetricSnowflakeQueuedOverloadTimeAvg(mb.config.Metrics.SnowflakeQueuedOverloadTimeAvg),
		metricSnowflakeQueuedProvisioningTimeAvg:             newMetricSnowflakeQueuedProvisioningTimeAvg(mb.config.Metrics.SnowflakeQueuedProvisioningTimeAvg),
		metricSnowflakeQueuedRepairTimeAvg:                   newMetricSnowflakeQueuedRepairTimeAvg(mb.config.Metrics.SnowflakeQueuedRepairTimeAvg),
		metricSnowflakeRowsDeletedAvg:                        newMetricSnowflakeRowsDeletedAvg(mb.config.Metrics.SnowflakeRowsDeletedAvg),
		metricSnowflakeRowsInsertedAvg:                       newMetricSnowflakeRowsInsertedAvg(mb.config.Metrics.SnowflakeRowsInsertedAvg),
		metricSnowflakeRowsProducedAvg:                       newMetricSnowflakeRowsProducedAvg(mb.config.Metrics.SnowflakeRowsProducedAvg),
		metricSnowflakeRowsUnloadedAvg:                       newMetricSnowflakeRowsUnloadedAvg(mb.config.Metrics.SnowflakeRowsUnloadedAvg),
		metricSnowflakeRowsUpdatedAvg:                        newMetricSnowflakeRowsUpdatedAvg(mb.config.Metrics.SnowflakeRowsUpdatedAvg),
		metricSnowflakeSessionIDCount:                        newMetricSnowflakeSessionIDCount(mb.config.Metrics.SnowflakeSessionIDCount),
		metricSnowflakeStorageFailsafeBytesTotal:             newMetricSnowflakeStorageFailsafeBytesTotal(mb.config.Metrics.SnowflakeStorageFailsafeBytesTotal),
		metricSnowflakeStorageStageBytesTotal:                newMetricSnowflakeStorageStageBytesTotal(mb.config.Metrics.SnowflakeStorageStageBytesTotal),
		metricSnowflakeStorageStorageBytesTotal:              newMetricSnowflakeStorageStorageBytesTotal(mb.config.Metrics.SnowflakeStorageStorageBytesTotal),
		metricSnowflakeTotalElapsedTimeAvg:                   newMetricSnowflakeTotalElapsedTimeAvg(mb.config.Metrics.SnowflakeTotalElapsedTimeAvg),
	}
	for _, op := range options {
		op(rmb)
	}
	mb.rmbMap[hash] = rmb
	return rmb
}

// NewResourceBuilder returns a new resource builder that should be used to build a resource associated with for the emitted metrics.
func (mb *MetricsBuilder) NewResourceBuilder() *ResourceBuilder {
	return NewResourceBuilder(mb.config.ResourceAttributes)
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (rmb *ResourceMetricsBuilder) updateCapacity(ms pmetric.MetricSlice) {
	if rmb.metricsCapacity < ms.Len() {
		rmb.metricsCapacity = ms.Len()
	}
}

// emit emits all the metrics accumulated by the ResourceMetricsBuilder and updates the internal state to be ready for
// recording another set of metrics. It returns true if any metrics were emitted.
func (rmb *ResourceMetricsBuilder) emit(m pmetric.Metrics) bool {
	sm := pmetric.NewScopeMetrics()
	sm.Metrics().EnsureCapacity(rmb.metricsCapacity)
	rmb.metricSnowflakeBillingCloudServiceTotal.emit(sm.Metrics())
	rmb.metricSnowflakeBillingTotalCreditTotal.emit(sm.Metrics())
	rmb.metricSnowflakeBillingVirtualWarehouseTotal.emit(sm.Metrics())
	rmb.metricSnowflakeBillingWarehouseCloudServiceTotal.emit(sm.Metrics())
	rmb.metricSnowflakeBillingWarehouseTotalCreditTotal.emit(sm.Metrics())
	rmb.metricSnowflakeBillingWarehouseVirtualWarehouseTotal.emit(sm.Metrics())
	rmb.metricSnowflakeDatabaseBytesScannedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeDatabaseQueryCount.emit(sm.Metrics())
	rmb.metricSnowflakeLoginsTotal.emit(sm.Metrics())
	rmb.metricSnowflakePipeCreditsUsedTotal.emit(sm.Metrics())
	rmb.metricSnowflakeQueryBlocked.emit(sm.Metrics())
	rmb.metricSnowflakeQueryBytesDeletedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryBytesSpilledLocalAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryBytesSpilledRemoteAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryBytesWrittenAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryCompilationTimeAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryDataScannedCacheAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryExecuted.emit(sm.Metrics())
	rmb.metricSnowflakeQueryExecutionTimeAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryPartitionsScannedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueryQueuedOverload.emit(sm.Metrics())
	rmb.metricSnowflakeQueryQueuedProvision.emit(sm.Metrics())
	rmb.metricSnowflakeQueuedOverloadTimeAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueuedProvisioningTimeAvg.emit(sm.Metrics())
	rmb.metricSnowflakeQueuedRepairTimeAvg.emit(sm.Metrics())
	rmb.metricSnowflakeRowsDeletedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeRowsInsertedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeRowsProducedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeRowsUnloadedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeRowsUpdatedAvg.emit(sm.Metrics())
	rmb.metricSnowflakeSessionIDCount.emit(sm.Metrics())
	rmb.metricSnowflakeStorageFailsafeBytesTotal.emit(sm.Metrics())
	rmb.metricSnowflakeStorageStageBytesTotal.emit(sm.Metrics())
	rmb.metricSnowflakeStorageStorageBytesTotal.emit(sm.Metrics())
	rmb.metricSnowflakeTotalElapsedTimeAvg.emit(sm.Metrics())
	if sm.Metrics().Len() == 0 {
		return false
	}
	rmb.updateCapacity(sm.Metrics())
	sm.Scope().SetName("otelcol/snowflakereceiver")
	sm.Scope().SetVersion(rmb.buildInfo.Version)
	rm := m.ResourceMetrics().AppendEmpty()
	rmb.resource.CopyTo(rm.Resource())
	sm.MoveTo(rm.ScopeMetrics().AppendEmpty())
	return true
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user config, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit() pmetric.Metrics {
	m := pmetric.NewMetrics()
	for _, rmb := range mb.rmbMap {
		if ok := rmb.emit(m); !ok {
			rmb.missedEmits++
		}
	}
	for k, rmb := range mb.rmbMap {
		if rmb.missedEmits >= missedEmitsToDropRMB {
			delete(mb.rmbMap, k)
		}
	}
	return m
}

// RecordSnowflakeBillingCloudServiceTotalDataPoint adds a data point to snowflake.billing.cloud_service.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingCloudServiceTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	rmb.metricSnowflakeBillingCloudServiceTotal.recordDataPoint(rmb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingTotalCreditTotalDataPoint adds a data point to snowflake.billing.total_credit.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingTotalCreditTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	rmb.metricSnowflakeBillingTotalCreditTotal.recordDataPoint(rmb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingVirtualWarehouseTotalDataPoint adds a data point to snowflake.billing.virtual_warehouse.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingVirtualWarehouseTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	rmb.metricSnowflakeBillingVirtualWarehouseTotal.recordDataPoint(rmb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingWarehouseCloudServiceTotalDataPoint adds a data point to snowflake.billing.warehouse.cloud_service.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingWarehouseCloudServiceTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeBillingWarehouseCloudServiceTotal.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseTotalCreditTotalDataPoint adds a data point to snowflake.billing.warehouse.total_credit.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingWarehouseTotalCreditTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeBillingWarehouseTotalCreditTotal.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseVirtualWarehouseTotalDataPoint adds a data point to snowflake.billing.warehouse.virtual_warehouse.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeBillingWarehouseVirtualWarehouseTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeBillingWarehouseVirtualWarehouseTotal.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeDatabaseBytesScannedAvgDataPoint adds a data point to snowflake.database.bytes_scanned.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeDatabaseBytesScannedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeDatabaseBytesScannedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeDatabaseQueryCountDataPoint adds a data point to snowflake.database.query.count metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeDatabaseQueryCountDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeDatabaseQueryCount.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeLoginsTotalDataPoint adds a data point to snowflake.logins.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeLoginsTotalDataPoint(ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	rmb.metricSnowflakeLoginsTotal.recordDataPoint(rmb.startTime, ts, val, errorMessageAttributeValue, reportedClientTypeAttributeValue, isSuccessAttributeValue)
}

// RecordSnowflakePipeCreditsUsedTotalDataPoint adds a data point to snowflake.pipe.credits_used.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakePipeCreditsUsedTotalDataPoint(ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	rmb.metricSnowflakePipeCreditsUsedTotal.recordDataPoint(rmb.startTime, ts, val, pipeNameAttributeValue)
}

// RecordSnowflakeQueryBlockedDataPoint adds a data point to snowflake.query.blocked metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryBlockedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeQueryBlocked.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryBytesDeletedAvgDataPoint adds a data point to snowflake.query.bytes_deleted.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryBytesDeletedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryBytesDeletedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledLocalAvgDataPoint adds a data point to snowflake.query.bytes_spilled.local.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryBytesSpilledLocalAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryBytesSpilledLocalAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledRemoteAvgDataPoint adds a data point to snowflake.query.bytes_spilled.remote.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryBytesSpilledRemoteAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryBytesSpilledRemoteAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesWrittenAvgDataPoint adds a data point to snowflake.query.bytes_written.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryBytesWrittenAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryBytesWrittenAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryCompilationTimeAvgDataPoint adds a data point to snowflake.query.compilation_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryCompilationTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryCompilationTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryDataScannedCacheAvgDataPoint adds a data point to snowflake.query.data_scanned_cache.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryDataScannedCacheAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryDataScannedCacheAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryExecutedDataPoint adds a data point to snowflake.query.executed metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryExecutedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeQueryExecuted.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryExecutionTimeAvgDataPoint adds a data point to snowflake.query.execution_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryExecutionTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryExecutionTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryPartitionsScannedAvgDataPoint adds a data point to snowflake.query.partitions_scanned.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryPartitionsScannedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueryPartitionsScannedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryQueuedOverloadDataPoint adds a data point to snowflake.query.queued_overload metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryQueuedOverloadDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeQueryQueuedOverload.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryQueuedProvisionDataPoint adds a data point to snowflake.query.queued_provision metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueryQueuedProvisionDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	rmb.metricSnowflakeQueryQueuedProvision.recordDataPoint(rmb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueuedOverloadTimeAvgDataPoint adds a data point to snowflake.queued_overload_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueuedOverloadTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueuedOverloadTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedProvisioningTimeAvgDataPoint adds a data point to snowflake.queued_provisioning_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueuedProvisioningTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueuedProvisioningTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedRepairTimeAvgDataPoint adds a data point to snowflake.queued_repair_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeQueuedRepairTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeQueuedRepairTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsDeletedAvgDataPoint adds a data point to snowflake.rows_deleted.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeRowsDeletedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeRowsDeletedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsInsertedAvgDataPoint adds a data point to snowflake.rows_inserted.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeRowsInsertedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeRowsInsertedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsProducedAvgDataPoint adds a data point to snowflake.rows_produced.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeRowsProducedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeRowsProducedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUnloadedAvgDataPoint adds a data point to snowflake.rows_unloaded.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeRowsUnloadedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeRowsUnloadedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUpdatedAvgDataPoint adds a data point to snowflake.rows_updated.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeRowsUpdatedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeRowsUpdatedAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeSessionIDCountDataPoint adds a data point to snowflake.session_id.count metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeSessionIDCountDataPoint(ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	rmb.metricSnowflakeSessionIDCount.recordDataPoint(rmb.startTime, ts, val, userNameAttributeValue)
}

// RecordSnowflakeStorageFailsafeBytesTotalDataPoint adds a data point to snowflake.storage.failsafe_bytes.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeStorageFailsafeBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	rmb.metricSnowflakeStorageFailsafeBytesTotal.recordDataPoint(rmb.startTime, ts, val)
}

// RecordSnowflakeStorageStageBytesTotalDataPoint adds a data point to snowflake.storage.stage_bytes.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeStorageStageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	rmb.metricSnowflakeStorageStageBytesTotal.recordDataPoint(rmb.startTime, ts, val)
}

// RecordSnowflakeStorageStorageBytesTotalDataPoint adds a data point to snowflake.storage.storage_bytes.total metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeStorageStorageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	rmb.metricSnowflakeStorageStorageBytesTotal.recordDataPoint(rmb.startTime, ts, val)
}

// RecordSnowflakeTotalElapsedTimeAvgDataPoint adds a data point to snowflake.total_elapsed_time.avg metric.
func (rmb *ResourceMetricsBuilder) RecordSnowflakeTotalElapsedTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	rmb.metricSnowflakeTotalElapsedTimeAvg.recordDataPoint(rmb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// Reset resets the ResourceMetricsBuilder to its initial state. It should be used when external metrics source is
// restarted, and the ResourceMetricsBuilder should update its startTime and reset it's internal state accordingly.
func (rmb *ResourceMetricsBuilder) Reset(options ...resourceMetricsBuilderOption) {
	rmb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(rmb)
	}
}
